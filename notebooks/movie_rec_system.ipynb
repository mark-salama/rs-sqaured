{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import BaselineOnly\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate,train_test_split,KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io  # needed because of weird encoding of u.item file\n",
    "\n",
    "from surprise import KNNBaseline\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNBasic\n",
    "from surprise import get_dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import NMF\n",
    "from surprise import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import large ratings file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.expanduser('~/Downloads/ml-20m/ratings.csv')\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',',skip_lines=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_ratings = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_ratings.drop('timestamp',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg rating for movies with more than 250 reviews\n",
    "avg_rating = large_ratings.groupby('movieId').mean()\\\n",
    "[large_ratings.groupby('movieId').count()['userId'] > 250].reset_index()[['movieId','rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_inner_id.raw_id = movie_id_inner_id.raw_id.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rating['movie_name'] = avg_rating.movieId.apply(lambda x: item_name_dict[str(x)])\n",
    "\n",
    "avg_rating = avg_rating.merge(movie_id_inner_id, left_on='movieId', right_on='raw_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the inner id (by reseting index) which serves as the matrix row, so how we go from row to movie name\n",
    "movie_dict_reduced = avg_rating.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict_reduced = movie_dict_reduced[['index','movie_name','rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict_reduced.columns = ['matrix_row','movie_name','avg_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Mark/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "movie_dict_reduced['year'] = movie_dict_reduced.movie_name.apply(return_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict_reduced['avg_rating'] = movie_dict_reduced['avg_rating'].map(lambda x: '%1.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matrix_row</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6137</th>\n",
       "      <td>6137</td>\n",
       "      <td>The Imitation Game (2014)</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6138</th>\n",
       "      <td>6138</td>\n",
       "      <td>The Hunger Games: Mockingjay - Part 1 (2014)</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6139</th>\n",
       "      <td>6139</td>\n",
       "      <td>Theory of Everything, The (2014)</td>\n",
       "      <td>3.72</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6140</th>\n",
       "      <td>6140</td>\n",
       "      <td>The Hobbit: The Battle of the Five Armies (2014)</td>\n",
       "      <td>3.41</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6141</th>\n",
       "      <td>6141</td>\n",
       "      <td>The Interview (2014)</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      matrix_row                                        movie_name avg_rating  \\\n",
       "6137        6137                         The Imitation Game (2014)       3.98   \n",
       "6138        6138      The Hunger Games: Mockingjay - Part 1 (2014)       3.45   \n",
       "6139        6139                  Theory of Everything, The (2014)       3.72   \n",
       "6140        6140  The Hobbit: The Battle of the Five Armies (2014)       3.41   \n",
       "6141        6141                              The Interview (2014)       2.90   \n",
       "\n",
       "      year  \n",
       "6137  2014  \n",
       "6138  2014  \n",
       "6139  2014  \n",
       "6140  2014  \n",
       "6141  2014  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_dict_reduced.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict_reduced[['movie_name','avg_rating','year']].to_csv('movie_dict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0,10).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26744, 15)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6142, 10)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_pca_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_pca_reduced = sim_pca[avg_rating.inner_id.values,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"movie_factors.csv\", sim_pca_reduced, delimiter=\",\",fmt='%1.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import small ratings file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to dataset file\n",
    "# file_path = os.path.expanduser('~/Downloads/ml-20m/ratings.csv')\n",
    "file_path = os.path.expanduser('~/Downloads/ml-latest-small/ratings.csv')\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',',skip_lines=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ratings = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_name_dict = {}\n",
    "name_item_dict = {}\n",
    "\n",
    "# file_path = os.path.expanduser('~/Downloads/ml-latest-small/movies.csv')\n",
    "file_path = os.path.expanduser('~/Downloads/ml-20m/movies.csv')\n",
    "\n",
    "with open(file_path, \"r\") as csvfile:\n",
    "    ff = csv.reader(csvfile)\n",
    "    for row in ff:\n",
    "        item_name_dict[row[0]] = row[1]\n",
    "        name_item_dict[row[1]] = row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieId_avg_rating = small_ratings.groupby('movieId').mean().reset_index()[['movieId','rating']]\n",
    "movieId_avg_rating_dict = {row.movieId:row.rating for index,row in movieId_avg_rating.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userId_avg_rating = small_ratings.groupby('userId').mean().reset_index()[['userId','rating']]\n",
    "userId_avg_rating_dict = {row.userId:row.rating for index,row in userId_avg_rating.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_center_movie(row):\n",
    "    return row['rating'] - movieId_avg_rating_dict[row['movieId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_center(row):\n",
    "    return row['rating'] - userId_avg_rating_dict[row['userId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ratings['rating_adj'] = small_ratings.apply(mean_center,axis=1)\n",
    "# small_ratings['rating_adj'] = small_ratings.apply(lambda x: x['rating'] - userId_rating_dict[x['userId']] + 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ratings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_to_userId = {x:y for x,y in enumerate(small_ratings.userId.unique())}\n",
    "userId_to_row = {y:x for x,y in enumerate(small_ratings.userId.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_movieId = {x:y for x,y in enumerate(small_ratings.movieId.unique())}\n",
    "movieId_to_col = {y:x for x,y in enumerate(small_ratings.movieId.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean =  sparse.csr_matrix((small_ratings['rating_adj'],\\\n",
    "                        ([userId_to_row[x] for x in small_ratings['userId']],\\\n",
    "                          [movieId_to_col[x] for x in small_ratings['movieId']])), shape=(671, 9066))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  sparse.csr_matrix((small_ratings['rating'],\\\n",
    "                        ([userId_to_row[x] for x in small_ratings['userId']],\\\n",
    "                          [movieId_to_col[x] for x in small_ratings['movieId']])), shape=(671, 9066))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean_na = X_mean.toarray()\n",
    "X_mean_na[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in small_ratings.iterrows():\n",
    "    X_mean_na[userId_to_row[row.userId],movieId_to_col[row.movieId]] = row.rating_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan with item mean\n",
    "X_array_mean_filled = X.toarray()\n",
    "\n",
    "# column mean\n",
    "col_mean = np.true_divide(X_array_mean_filled.sum(0),(X_array_mean_filled!=0).sum(0))\n",
    "\n",
    "#Find indicies that you need to replace\n",
    "inds = np.where(X_array_mean_filled == 0)\n",
    "\n",
    "#Place column means in the indices. Align the arrays using take\n",
    "X_array_mean_filled[inds] = np.take(col_mean, inds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add year and rating to movie data\n",
    "small_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict = pd.DataFrame({'movie_id':small_ratings.movieId.unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict['matrix_row'] = movie_dict.movie_id.apply(lambda x: movieId_to_col[x])\n",
    "movie_dict['movie_name'] = movie_dict.movie_id.apply(lambda x: item_name_dict[str(x)])\n",
    "movie_dict['avg_rating'] = movie_dict.movie_id.apply(lambda x: movieId_avg_rating_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict.to_csv('movie_dict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_year(row):\n",
    "    \n",
    "    if any(i.isdigit() for i in row):\n",
    "    \n",
    "        year_pos = len(row.split('(')) - 1\n",
    "\n",
    "        year = row.split('(')[year_pos].split(')')[0]\n",
    "\n",
    "        if year.find('-'):\n",
    "            return int(year[:4])\n",
    "        else:\n",
    "            return int(year)\n",
    "    else:\n",
    "        return int(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict['year'] = movie_dict.movie_name.apply(return_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### similar movies comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_similar_movies(movie_vectors, movie_id, row_id):\n",
    "    \n",
    "    if movie_id == '':\n",
    "        column = row_id\n",
    "    else:\n",
    "        column = movieId_to_col[movie_id]\n",
    "    \n",
    "    similarities = cosine_similarity(movie_vectors)\n",
    "    \n",
    "    sim_movies = [[item_name_dict[str(col_to_movieId[x])],movieId_avg_rating_dict[col_to_movieId[x]]]\\\n",
    "                  for x in np.argsort(-similarities[column])[:20] if x < 9066]\n",
    "    \n",
    "    return sim_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieId_avg_rating_dict[int(441)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotted = matrix.dot(vector)\n",
    "matrix_norms = np.linalg.norm(matrix, axis=1)\n",
    "vector_norm = np.linalg.norm(vector)\n",
    "matrix_vector_norms = np.multiply(matrix_norms, vector_norm)\n",
    "neighbors = np.divide(dotted, matrix_vector_norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML HW 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinate ascent to build movie and user matrix factorizations\n",
    "def matrix_fact(numb_factor, iterations, user_item_matrix):\n",
    "    lamb = 1\n",
    "    varsq = .25\n",
    "    \n",
    "    user_m = np.random.multivariate_normal(np.zeros(numb_factor),np.eye(numb_factor),user_item_matrix.shape[0])\n",
    "    movie_m = np.random.multivariate_normal(np.zeros(numb_factor),np.eye(numb_factor),user_item_matrix.shape[1])\n",
    "    first_part = lamb*varsq*np.eye(numb_factor)\n",
    "\n",
    "    for t in range(0,iterations):\n",
    "\n",
    "        for j in range(0,user_m.shape[0]):\n",
    "\n",
    "            locas_nan = np.where(~np.isnan(user_item_matrix[j,:]))\n",
    "            v_dot = np.dot(np.transpose(movie_m[locas_nan]),movie_m[locas_nan])\n",
    "            m_v_dot = np.dot(user_item_matrix[j][locas_nan],movie_m[locas_nan])         \n",
    "            user_m[j] = (np.dot(np.linalg.inv(first_part+v_dot),m_v_dot)).flatten()\n",
    "\n",
    "        for k in range(0,movie_m.shape[0]):\n",
    "\n",
    "            locas_nan = np.where(~np.isnan(user_item_matrix[:,k]))\n",
    "            u_dot = np.dot(np.transpose(user_m[locas_nan]),user_m[locas_nan])\n",
    "            m_u_dot = np.dot(user_item_matrix[:,k][locas_nan],user_m[locas_nan])\n",
    "            movie_m[k] = (np.dot(np.linalg.inv(first_part+u_dot),m_u_dot)).flatten()\n",
    "        \n",
    "    return user_m, movie_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vectors, movie_vectors = matrix_fact(15,150,X_mean_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = 0\n",
    "count = 0\n",
    "for index,row in small_ratings.iterrows():\n",
    "    \n",
    "    guess = np.dot(user_vectors[userId_to_row[row[0]]],movie_vectors[movieId_to_col[row[1]]])\n",
    "    RMSE += (float(row[4])-guess)**2\n",
    "    count += 1\n",
    "\n",
    "RMSE_run = math.sqrt(RMSE/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"movie_factors_test.csv\", movie_vectors, delimiter=\",\",fmt='%1.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_similar_movies(movie_vectors,3,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_name_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=15, init='nndsvda', random_state=0, verbose=True, solver='cd', alpha = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = model.fit_transform(X.toarray())\n",
    "H = model.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_similar_movies(H,1,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=10, init='nndsvda', random_state=0, verbose=True, solver='cd', alpha = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_mean_filled_cols = model.fit_transform(X_array_mean_filled)\n",
    "H_mean_filled_cols = model.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_similar_movies(H_mean_filled_cols,344,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling in the column mean makes the results worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### matrix math - R.T * R to see where people who like one movie, like another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean_positive = (X_mean.toarray() > 0) * X_mean.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrix_product = np.dot(X_mean_positive.T,X_mean_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_highest_movie_in_column(matrix,movie_id):\n",
    "    \n",
    "    sim_movies = [item_name_dict[str(col_to_movieId[x])] for x in np.argsort(-matrix[:,movieId_to_col[movie_id]])[:20] if x < len(item_name_dict)]\n",
    "\n",
    "    return sim_movies\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_count_matrix = np.dot((X_mean.toarray() > 0).astype(int).T,(X_mean.toarray() != 0).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = np.log(9066/np.diag(tf_idf_count_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_results = tf_idf_count_matrix * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_results[tf_idf_results == np.inf] = 0\n",
    "tf_idf_results = np.nan_to_num(tf_idf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_highest_movie_in_column(X_matrix_product,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### surprise similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_similar_movies_surprise(movie_vectors, movie_id, row_id):\n",
    "    \n",
    "    if movie_id == '':\n",
    "        column = row_id\n",
    "    else:\n",
    "        column = algo.trainset.to_inner_iid(movie_id)\n",
    "    \n",
    "    similarities = cosine_similarity(movie_vectors)\n",
    "    \n",
    "    sim_movies = [item_name_dict[str(algo.trainset.to_raw_iid(x))]\\\n",
    "                  for x in np.argsort(-similarities[column])[:20]]\n",
    "    \n",
    "    return sim_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.expanduser('~/Downloads/ml-20m/ratings.csv')\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',',skip_lines=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.load_from_file(file_path, reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = data.build_full_trainset()                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearson baseline > pearson > cosine > msd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBasic at 0x10878c898>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_options = {'name': 'pearson_baseline', 'user_based': False}\n",
    "# algo = KNNBaseline(sim_options=sim_options)\n",
    "algo = KNNBasic(sim_options=sim_options)\n",
    "\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_neighbors_from_sim(algo,movie_id):\n",
    "    # Retrieve inner id of the movie Toy Story\n",
    "\n",
    "    movie_name = item_name_dict[str(movie_id)]\n",
    "    print('starting_movie: {}'.format(movie_name))\n",
    "    \n",
    "    movie_raw_id = name_item_dict[movie_name]\n",
    "    movie_inner_id = algo.trainset.to_inner_iid(movie_raw_id)\n",
    "\n",
    "    # Retrieve inner ids of the nearest neighbors of Toy Story.\n",
    "    movie_neighbors = algo.get_neighbors(movie_inner_id, k=20)\n",
    "\n",
    "    # Convert inner ids of the neighbors into names.\n",
    "    movie_neighbors = [algo.trainset.to_raw_iid(inner_id)\n",
    "                           for inner_id in movie_neighbors]\n",
    "    movie_neighbors = [item_name_dict[rid]\n",
    "                           for rid in movie_neighbors]\n",
    "    return movie_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_neighbors_from_sim(algo,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surprise similarity is really good but the matrix is too big too store, so trying PCA\n",
    "# try PCA on sim matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.compute_similarities().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"ml_20m_sim_pca_matrix.csv\", sim_pca, delimiter=\",\",fmt='%1.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.3618260207672808\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=15)\n",
    "sim_pca = pca.fit_transform(algo.compute_similarities())\n",
    "print(pca.explained_variance_ratio_.sum())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_inner_id = pd.DataFrame.from_dict({key:algo.trainset.to_raw_iid(key) for key in algo.trainset.all_items()}, orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_inner_id.columns = ['inner_id','raw_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_inner_id.to_csv('ml_20m_sim_pca_matrix_movie_id_inner_id_dict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lion King, The (1994)',\n",
       " 'Aladdin (1992)',\n",
       " 'Romancing the Stone (1984)',\n",
       " 'Big (1988)',\n",
       " 'Field of Dreams (1989)',\n",
       " 'Little Mermaid, The (1989)',\n",
       " 'Beauty and the Beast (1991)',\n",
       " 'Star Trek IV: The Voyage Home (1986)',\n",
       " 'E.T. the Extra-Terrestrial (1982)',\n",
       " \"Bug's Life, A (1998)\",\n",
       " 'Apollo 13 (1995)',\n",
       " 'Cocoon (1985)',\n",
       " 'Splash (1984)',\n",
       " 'Back to the Future (1985)',\n",
       " 'Monsters, Inc. (2001)',\n",
       " 'Dances with Wolves (1990)',\n",
       " 'Hunchback of Notre Dame, The (1996)',\n",
       " 'City Slickers (1991)',\n",
       " 'When Harry Met Sally... (1989)',\n",
       " 'Superman (1978)']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_similar_movies_surprise(sim_pca[:,np.arange(0,12).tolist()],'364','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_similar_movies(movie_vectors,1,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVD()\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_similar_movies(algo.qi,10,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hypothetical user with just a 5 rating for given movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to dataset file\n",
    "# file_path = os.path.expanduser('~/Downloads/ml-20m/ratings.csv')\n",
    "file_path_adj = os.path.expanduser('~/Downloads/ml-latest-small/ratings_adj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "672\t1\t5\n",
    "673\t10\t5\n",
    "674\t344\t5\n",
    "675\t364\t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.load_from_file(file_path_adj, reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVD()\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Than predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "top_n = get_top_n(predictions, n=10)\n",
    "\n",
    "# Print the recommended items for each user\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combining vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_two_items(movie_factors,*args):\n",
    "    \n",
    "    for i in args:\n",
    "        print('movie_name: {}'.format((item_name_dict[str(i)])))\n",
    "    \n",
    "    item_cols = [movieId_to_col[int(x)] for x in args]\n",
    "    \n",
    "#     closest_cols = np.argsort(abs(movie_factors[item_1_col]-movie_factors[item_2_col]))[:10]\n",
    "#     new_matrix = movie_factors[:,closest_cols]\n",
    "    \n",
    "    new_matrix = movie_factors[:,:]\n",
    "    \n",
    "    print('max: {}'.format(new_matrix.shape[0]))\n",
    "    max_orig_array = np.maximum.reduce(movie_factors[item_cols])\n",
    "    new_matrix = np.append(new_matrix, np.array(max_orig_array).reshape(1,-1),axis = 0)\n",
    "    \n",
    "    print('min: {}'.format(new_matrix.shape[0])) \n",
    "    min_orig_array = np.minimum.reduce(movie_factors[item_cols])\n",
    "    new_matrix = np.append(new_matrix, np.array(min_orig_array).reshape(1,-1),axis = 0)\n",
    "    \n",
    "    print('mean: {}'.format(new_matrix.shape[0]))      \n",
    "    mean_of_items = np.mean(movie_factors[item_cols],axis=0)\n",
    "    new_matrix = np.append(new_matrix, mean_of_items.reshape(1,-1), axis=0)\n",
    "    \n",
    "    return new_matrix\n",
    "    \n",
    "    perc = np.percentile(movie_factors, np.arange(0,101,10),axis=0)\n",
    "    \n",
    "    print(perc.shape)\n",
    "    \n",
    "    movie_1 = []\n",
    "    movie_2 = []\n",
    "    movie_3 = []\n",
    "    \n",
    "    print(new_matrix.shape)\n",
    "    \n",
    "    for x in range(perc.shape[1]):\n",
    "        movie_1.append(np.digitize(movie_factors[item_1_col,x], perc[:,x]).flatten()[0])\n",
    "        movie_2.append(np.digitize(movie_factors[item_2_col,x], perc[:,x]).flatten()[0])\n",
    "        movie_3.append(np.digitize(movie_factors[item_3_col,x], perc[:,x]).flatten()[0])\n",
    "\n",
    "    max_array = []\n",
    "    for x,i in enumerate(np.maximum.reduce([movie_1,movie_2,movie_3])):\n",
    "        max_array.append(np.percentile(movie_factors[:,x],(i-1)*10))\n",
    "    \n",
    "    print(np.array(max_array).reshape(1,-1).shape)\n",
    "    \n",
    "    new_matrix = np.append(new_matrix, np.array(max_array).reshape(1,-1),axis = 0)\n",
    "   \n",
    "    min_array = []\n",
    "    for x,i in enumerate(np.minimum.reduce([movie_1,movie_2,movie_3])):\n",
    "        min_array.append(np.percentile(movie_factors[:,x],(i-1)*10))\n",
    "        \n",
    "    new_matrix = np.append(new_matrix, np.array(min_array).reshape(1,-1),axis = 0)    \n",
    "    \n",
    "    mean_of_items = np.mean([movie_factors[item_1_col,:],movie_factors[item_2_col,:],movie_factors[item_3_col,:]],axis=0)\n",
    "    \n",
    "    new_matrix = np.append(new_matrix, mean_of_items.reshape(1,-1), axis=0)\n",
    "    \n",
    "    min_vector = np.minimum.reduce(movie_factors[[item_1_col,item_2_col,item_3_col]])\n",
    "    \n",
    "    new_matrix = np.append(new_matrix, min_vector.reshape(1,-1), axis=0)\n",
    "    \n",
    "    summed_vector = np.zeros(new_matrix.shape[1])\n",
    "    \n",
    "    for i in [item_1_col,item_2_col,item_3_col]:\n",
    "        summed_vector += new_matrix[i]\n",
    "\n",
    "    \n",
    "    new_matrix = np.append(new_matrix, summed_vector.reshape(1,-1), axis=0)\n",
    "\n",
    "#     new_matrix_sparse = sparse.csr_matrix(new_matrix)\n",
    "#     new_matrix_sparse = sparse.csr_matrix(algo.qi)\n",
    "    \n",
    "#     similarities = cosine_similarity(new_matrix)\n",
    "    \n",
    "#     [[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(first_try[9066])[-20:-1]]\n",
    "    \n",
    "\n",
    "    \n",
    "    return new_matrix\n",
    "    \n",
    "#     return new_matrix,movie_factors[item_1_col],movie_factors[item_2_col],movie_factors[item_3_col],movie_1,movie_2,movie_3\n",
    "\n",
    "\n",
    "# A =  np.array([[0, 1, 0, 0, 1], [0, 0, 1, 1, 1],[1, 1, 0, 1, 0]])\n",
    "# A_sparse = sparse.csr_matrix(A)\n",
    "\n",
    "# similarities = cosine_similarity(A_sparse)\n",
    "# print('pairwise dense output:\\n {}\\n'.format(similarities))\n",
    "\n",
    "# #also can output sparse matrices\n",
    "# similarities_sparse = cosine_similarity(A_sparse,dense_output=False)\n",
    "# print('pairwise sparse output:\\n {}\\n'.format(similarities_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_vectors = plot_two_items(sim_pca,1136,2788,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_similar_movies(combined_vectors,'',9068)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieId_avg_rating_dict[541]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_similar_movies(combined_vectors,'',9068)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion to percentiles and then minimum worked best for\n",
    "'Casper (1995)','Toy Story (1995)','Pocahontas (1995)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_vectors[9069]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_vectors[9068]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_item_dict['Toy Story (1995)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieId_to_col[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in top_n['676']:\n",
    "    print(item_name_dict[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ratings.userId.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_similarities(rows):\n",
    "    \n",
    "    min_vector = np.minimum.reduce(similarities[rows])\n",
    "    \n",
    "    summed_vector = np.zeros(similarities.shape[1])\n",
    "    for i in rows:\n",
    "        summed_vector += similarities[i]\n",
    "        print(item_name_dict[str(col_to_movieId[i])])\n",
    "    return summed_vector, min_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_vector, min_vector = sum_similarities([143,4939])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[4939]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[966][3679]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(-min_vector)[:20] if x < 9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(-similarities_orig[966])[:20] if x < 9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(-similarities_sur[966])[:20] if x < 9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import SparsePCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = SparsePCA(n_components=10)\n",
    "X_sparse_PCA = pca.fit_transform(X.toarray().T)\n",
    "print(pca.explained_variance_ratio_.sum())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = X_sparse_PCA\n",
    "vector = X_sparse_PCA[965]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(-neighbors)[:10] if x < 9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(-similarities[965])[:10] if x < 9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"movie_factors.csv\", movie_factors, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict = pd.DataFrame({'movie_id':small_ratings.movieId.unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict['matrix_row'] = movie_dict.movie_id.apply(lambda x: movieId_to_col[x])\n",
    "movie_dict['movie_name'] = movie_dict.movie_id.apply(lambda x: item_name_dict[str(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict.to_csv('movie_dict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt(\"movie_factors.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[test] == [movie_factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(movie_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_sur = cosine_similarity(algo.qi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = movie_factors\n",
    "vector = agg_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotted = matrix.dot(vector)\n",
    "matrix_norms = np.linalg.norm(matrix, axis=1)\n",
    "vector_norm = np.linalg.norm(vector)\n",
    "matrix_vector_norms = np.multiply(matrix_norms, vector_norm)\n",
    "neighbors = np.divide(dotted, matrix_vector_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matrix = np.append(movie_factors, agg_vector.reshape(1,-1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(new_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(similarities[9066])[-10:] if x < 9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(-neighbors)[:10] if x < 9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_movieId[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ratings.movieId.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'movie_factors':movie_factors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_value = ['1','100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = movie_dict[movie_dict.movie_id.isin(movie_value)].matrix_row.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_factors[417]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_factors[2071]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_factors[cols,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_vector = np.maximum.reduce(movie_factors[cols,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_factors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_factors[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.maximum.reduce([movie_factors[100],movie_factors[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_try,movie_1,movie_2,movie_3,movie_1_perc,movie_2_perc,movie_3_perc = plot_two_items(movie_factors,'Casper (1995)','Toy Story (1995)','Pocahontas (1995)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_name_dict[str(col_to_movieId[72])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(first_try[movieId_to_col[719]])[-10:] if x < 9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(first_try[movieId_to_col[539]])[-10:] if x < 9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(first_try[movieId_to_col[10]])[-10:] if x < 9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max from original arrays\n",
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(first_try[9066])[-10:] if x < 9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max from percentiles of original arrays\n",
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(first_try[9067])[-10:] if x < 9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min from percentiles of original arrays\n",
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(first_try[9068])[-10:] if x < 9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of all arrays\n",
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(first_try[9069])[-10:] if x < 9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in item_name_dict:\n",
    "    print('{',\"\"\"'label': \"{}\", 'value': {}\"\"\".format(item_name_dict[key],str(key)),'},')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame([item_name_dict]).T.reset_index()\n",
    "sample.columns = ['movieId','movie_name']\n",
    "sample.to_csv('movie_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.movieId == '148'].movie_name.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.movie_name.head():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dash_table(df):\n",
    "    ''' Return a dash definition of an HTML table for a Pandas dataframe '''\n",
    "    table = []\n",
    "    for index, row in df.iterrows():\n",
    "        html_row = []\n",
    "        for i in range(len(row)):\n",
    "            html_row.append(html.Td([row[i]]))\n",
    "        table.append(html.Tr(html_row))\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_1_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_2_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(movie_1_perc,movie_2_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.maximum.reduce([movie_1_perc,movie_2_perc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = np.array()\n",
    "for x,i in enumerate(np.maximum.reduce([movie_1_perc,movie_2_perc])):\n",
    "    new_array.append(np.percentile(movie_factors[:,x],(i-1)*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(new_array).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,1]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_factors = [0,1]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(movie_factors.shape[1]):\n",
    "    print(movie_factors[:,i].mean(),movie_factors[:,i].min(),movie_factors[:,i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_1=[]\n",
    "movie_2=[]\n",
    "for x in perc.shape[1]:\n",
    "    movie_1.append(np.digitize(movie_factors, perc[:,x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,101,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = np.percentile(movie_factors[:,0], np.arange(0,101,10))\n",
    "indices = np.digitize(movie_factors[:,0], perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = np.percentile(movie_factors, np.arange(0,101,10),axis=0)\n",
    "perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_factors[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.digitize(movie_factors[0,0], perc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_factors[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_factors[:,0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(movie_factors[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_id = name_item_dict['Clueless (1995)']\n",
    "\n",
    "inner_id = algo.trainset.to_inner_iid(raw_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x,item_name_dict[str(col_to_movieId[x])]] for x in np.argsort(first_try[9066])[-20:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_try[9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movieId_to_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.load_from_file(file_path, reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = data.build_full_trainset()                                         \n",
    "algo.train(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVD(n_factors = 10)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.qi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matrix_sparse = sparse.csr_matrix(algo.qi)\n",
    "\n",
    "similarities = cosine_similarity(new_matrix_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(-similarities[988]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_id = name_item_dict['Ponyo (Gake no ue no Ponyo) (2008)']\n",
    "# raw_id = name_item_dict['Clueless (1995)']\n",
    "raw_id = name_item_dict['Toy Story (1995)']\n",
    "\n",
    "inner_id = algo.trainset.to_inner_iid(raw_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x,item_name_dict[algo.trainset.to_raw_iid(x)]] for x in np.argsort(-similarities[inner_id])[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'pearson_baseline', 'user_based': False}\n",
    "algo = KNNBaseline(sim_options=sim_options)\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve inner id of the movie Toy Story\n",
    "toy_story_raw_id = name_item_dict['Clueless (1995)']\n",
    "toy_story_inner_id = algo.trainset.to_inner_iid(toy_story_raw_id)\n",
    "\n",
    "# Retrieve inner ids of the nearest neighbors of Toy Story.\n",
    "toy_story_neighbors = algo.get_neighbors(toy_story_inner_id, k=10)\n",
    "\n",
    "# Convert inner ids of the neighbors into names.\n",
    "toy_story_neighbors = [algo.trainset.to_raw_iid(inner_id)\n",
    "                       for inner_id in toy_story_neighbors]\n",
    "toy_story_neighbors = [item_name_dict[rid]\n",
    "                       for rid in toy_story_neighbors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_story_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "\n",
    "algo = SVD()\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    accuracy.rmse(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'pearson_baseline', 'user_based': False}\n",
    "algo = KNNBaseline(sim_options=sim_options)\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.compute_similarities().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(algo.qi[0])\n",
    "# plt.plot(algo.qi[1])\n",
    "plt.plot(algo.qi[0]-algo.qi[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_cols = np.argsort(abs(algo.qi[0]-algo.qi[1]))[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.qi[0,closest_cols],algo.qi[1,closest_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([algo.qi[0,closest_cols],algo.qi[1,closest_cols]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matrix = algo.qi[:,closest_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_1_raw_id = name_item_dict[item_1]\n",
    "item_2_raw_id = name_item_dict[item_2]\n",
    "\n",
    "item_1_inner_id = algo.trainset.to_inner_iid(item_1_raw_id)\n",
    "item_2_inner_id = algo.trainset.to_inner_iid(item_2_raw_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_of_items.reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matrix = algo.qi[:,closest_cols]\n",
    "\n",
    "mean_of_items = np.mean([algo.qi[0,closest_cols],algo.qi[1,closest_cols]],axis=0)\n",
    "\n",
    "np.append(new_matrix, mean_of_items.reshape(1,-1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_two_items(algo,item_1,item_2):\n",
    "    item_1_raw_id = name_item_dict[item_1]\n",
    "    item_2_raw_id = name_item_dict[item_2]\n",
    "\n",
    "    item_1_inner_id = algo.trainset.to_inner_iid(item_1_raw_id)\n",
    "    item_2_inner_id = algo.trainset.to_inner_iid(item_2_raw_id)\n",
    "    \n",
    "    closest_cols = np.argsort(abs(algo.qi[item_1_inner_id]-algo.qi[item_2_inner_id]))[:100]\n",
    "    new_matrix = algo.qi[:,closest_cols]\n",
    "    \n",
    "    mean_of_items = np.mean([algo.qi[0,closest_cols],algo.qi[1,closest_cols]],axis=0)\n",
    "    \n",
    "#     new_matrix = np.append(new_matrix, mean_of_items.reshape(1,-1), axis=0)\n",
    "    \n",
    "#     new_matrix_sparse = sparse.csr_matrix(new_matrix)\n",
    "    new_matrix_sparse = sparse.csr_matrix(algo.qi)\n",
    "    \n",
    "    similarities = cosine_similarity(new_matrix_sparse)\n",
    "        \n",
    "    return similarities\n",
    "\n",
    "\n",
    "# A =  np.array([[0, 1, 0, 0, 1], [0, 0, 1, 1, 1],[1, 1, 0, 1, 0]])\n",
    "# A_sparse = sparse.csr_matrix(A)\n",
    "\n",
    "# similarities = cosine_similarity(A_sparse)\n",
    "# print('pairwise dense output:\\n {}\\n'.format(similarities))\n",
    "\n",
    "# #also can output sparse matrices\n",
    "# similarities_sparse = cosine_similarity(A_sparse,dense_output=False)\n",
    "# print('pairwise sparse output:\\n {}\\n'.format(similarities_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_try = plot_two_items(algo,'Leaving Las Vegas (1995)','Clueless (1995)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_id = name_item_dict['Clueless (1995)']\n",
    "\n",
    "inner_id = algo.trainset.to_inner_iid(raw_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x,item_name_dict[algo.trainset.to_raw_iid(x)]] for x in np.argsort(first_try[988])[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.qi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(algo.qi[988] - algo.qi[988]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.qi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matrix_sparse = sparse.csr_matrix(algo.qi)\n",
    "\n",
    "similarities = cosine_similarity(new_matrix_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(-similarities[988]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_id = name_item_dict['Ponyo (Gake no ue no Ponyo) (2008)']\n",
    "\n",
    "inner_id = algo.trainset.to_inner_iid(raw_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x,item_name_dict[algo.trainset.to_raw_iid(x)]] for x in np.argsort(-similarities[1503])[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.trainset.to_inner_iid('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_try.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_name_dict[algo.trainset.to_raw_iid(988)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_try[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_name_dict[algo.trainset.to_raw_iid[np.argsort(first_try[inner_id])[:10][0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.trainset.to_raw_iid(7078)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_try.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.qia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca.fit(algo.qi)\n",
    "print(pca.explained_variance_ratio_.sum())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.trainset.to_raw_iid(6065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve inner id of the movie Toy Story\n",
    "toy_story_raw_id = name_item_dict['Toy Story (1995)']\n",
    "toy_story_inner_id = algo.trainset.to_inner_iid(toy_story_raw_id)\n",
    "\n",
    "# Retrieve inner ids of the nearest neighbors of Toy Story.\n",
    "toy_story_neighbors = algo.get_neighbors(toy_story_inner_id, k=10)\n",
    "\n",
    "# Convert inner ids of the neighbors into names.\n",
    "toy_story_neighbors = [algo.trainset.to_raw_iid(inner_id)\n",
    "                       for inner_id in toy_story_neighbors]\n",
    "toy_story_neighbors = [item_name_dict[rid]\n",
    "                       for rid in toy_story_neighbors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_story_neighbors = algo.get_neighbors(toy_story_inner_id, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'480': 'Jurassic Park (1993)',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_story_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_story_inner_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_story_raw_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
